---
layout: post
title: The Summarize of kNN
tag: ML
---

# 更多有关k近邻算法

## 分类和回归

### 解决分类问题

天然可以解决多分类问题

思想简单，效果强大

### 解决回归问题

模型中数据的k个节点，计算平均值或者加权平均值

scikit-learn 中使用 KNeighborRegressor

### 缺点

#### 效率低下

如果训练m个样本，n个特征，预测每一个新数据，需要O(m*n),可以使用KD-Tree， Ball-Tree进行优化

#### 高度数据相关

对outlier更为敏感

#### 预测结果不具有可解释性

#### 维数灾难

随着维度的增加，“看似相近”的两个点之间的距离越来越大

维度 | 距离 | 大小
--- | --- | ---
1维|0到1的距离|1
2维|(0,0)到(1,1)的距离|1.414
3维|(0,0,0)到(1,1,1)的距离|1.73
64维|(0,0,...0)到(1,1,...1)的距离|8
10000维|(0,0,...0)到(1,1,...1)的距离|100

解决方法：降维

### 机器学习流程

1. 数据集分割 train test splite
2. 归一化
3. 使用网格搜索寻找最好的超参数
4. 训练模型 fit
5. 测试分类准确度
